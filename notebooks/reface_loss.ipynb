{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 23:20:02.666410: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-21 23:20:02.703751: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-21 23:20:02.703773: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-21 23:20:02.704668: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-21 23:20:02.710967: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-21 23:20:03.597320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from auramask.losses import PerceptualLoss, EmbeddingDistanceLoss\n",
    "from auramask.models import FaceEmbedEnum\n",
    "from keras.losses import CosineSimilarity, Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Victim Models (F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 23:20:05.764541: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-21 23:20:05.801128: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-21 23:20:05.802724: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-21 23:20:05.806360: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-21 23:20:05.807790: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-21 23:20:05.809272: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-21 23:20:05.962130: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-21 23:20:05.963619: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-21 23:20:05.965008: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-02-21 23:20:05.965049: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-21 23:20:05.966438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14406 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:00:04.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "F = [FaceEmbedEnum.ARCFACE, FaceEmbedEnum.DEEPFACE, FaceEmbedEnum.DEEPID]\n",
    "F_set = FaceEmbedEnum.build_F(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = 0.2\n",
    "# Backbones are 'vgg', 'alex', 'squeeze'\n",
    "backbone = 'vgg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings Loss\n",
    "$$\n",
    "loss \\leftarrow \\dfrac{1}{\\left\\|\\mathbb{F}\\right\\|} \\sum^{\\mathbb{F}}_{f} - \\dfrac{f(x) \\cdot f(x_{adv})} {\\left\\| f(x)\\right\\|_{2}\\left\\| f(x_{adv})\\right\\|_{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim = CosineSimilarity(axis=1)\n",
    "def f_cosine_similarity(x, x_adv, f):\n",
    "  emb_t = f(x)\n",
    "  emb_adv = f(x_adv)\n",
    "  dist = cossim(emb_t, emb_adv)\n",
    "  dist = tf.negative(dist)\n",
    "  return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(F)\n",
    "def F_loss(x, x_adv):\n",
    "  loss = 0.0\n",
    "  for f in F_set:\n",
    "    model = f[0]\n",
    "    aug = f[1]\n",
    "    loss = tf.add(loss, f_cosine_similarity(aug(x), aug(x_adv), model))\n",
    "  loss = tf.divide(loss, N)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptual Loss ($L_{pips}$)\n",
    "$$\n",
    "loss \\leftarrow loss + \\lambda L_{pips}(x_{adv}, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpips = PerceptualLoss(backbone=backbone, spatial=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reface_loss(x, x_adv):\n",
    "  floss = F_loss(x, x_adv)\n",
    "  lpipsloss = lpips(x, x_adv)\n",
    "  return tf.add(floss, lpipsloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Object\n",
    "How to use this particular loss function in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auramask.losses.reface import ReFaceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ReFaceLoss(F, backbone=backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ReFaceLoss',\n",
       " 'reduction': 'auto',\n",
       " 'PerceptualLoss': {'name': 'lpips', 'model': {}, 'reduction': 'auto'},\n",
       " 'EmbeddingDistanceLoss': {'name': 'EmbeddingsLoss',\n",
       "  'F': \"[<FaceEmbedEnum.ARCFACE: 'ArcFace'>, <FaceEmbedEnum.DEEPFACE: 'DeepFace'>, <FaceEmbedEnum.DEEPID: 'DeepID'>]\",\n",
       "  'reduction': 'auto'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "Testing the giving parts of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from keras_cv.layers import Resizing, Rescaling, Augmenter, RandomContrast, RandomColorDegeneration, RandomColorJitter\n",
    "from tensorflow.data import AUTOTUNE\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.\n"
     ]
    }
   ],
   "source": [
    "ds, info = tfds.load('lfw',\n",
    "                     decoders=tfds.decode.PartialDecoding({\n",
    "                       'image': True,\n",
    "                     }),\n",
    "                     with_info=True,\n",
    "                     download=True,\n",
    "                     as_supervised=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = int.from_bytes(\"Pochita Lives\".encode())\n",
    "\n",
    "augmenter = Augmenter(\n",
    "  [\n",
    "    Rescaling(1./255),\n",
    "    Resizing(128,128),\n",
    "  ]\n",
    ")\n",
    "\n",
    "noise_addition = Augmenter(\n",
    "  [\n",
    "    RandomContrast([0.0,1.0], [-0.5,0.5], seed=SEED),\n",
    "    RandomColorDegeneration([0,0.5], seed=SEED), \n",
    "    # RandomColorJitter(seed=SEED)\n",
    "  ]\n",
    ")\n",
    "\n",
    "def preprocess_data(images, augment=True):\n",
    "  inputs = {\"images\": images}\n",
    "  outputs_1 = augmenter(inputs)\n",
    "  if augment:\n",
    "    outputs_2 = noise_addition(inputs)\n",
    "  else:\n",
    "    outputs_2 = outputs_1\n",
    "  return outputs_1['images'], outputs_2['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(BATCH_SIZE).map(\n",
    "  lambda x: preprocess_data(x['image'], True),\n",
    "  num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Loss Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 23:22:00.842535: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-02-21 23:22:00.843814: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-02-21 23:22:00.902833: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-21 23:22:01.203383: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.93260753, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds.take(1):\n",
    "  print(loss(batch[0], batch[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses:\n",
      " \tF-Loss: 0.819\n",
      " \tLpips: 0.568\n",
      "\n",
      "Losses:\n",
      " \tF-Loss: 0.831\n",
      " \tLpips: 0.601\n",
      "\n",
      "Losses:\n",
      " \tF-Loss: 0.828\n",
      " \tLpips: 0.583\n",
      "\n",
      "Losses:\n",
      " \tF-Loss: 0.812\n",
      " \tLpips: 0.599\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 23:22:20.068981: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses:\n",
      " \tF-Loss: 0.829\n",
      " \tLpips: 0.583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds.take(5):\n",
    "  floss = F_loss(batch[0], batch[1])\n",
    "  ploss = lpips(batch[0], batch[1])\n",
    "  print(\"Losses:\\n\", \"\\tF-Loss: %0.3f\\n\"%(floss), \"\\tLpips: %0.3f\\n\"%(ploss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
